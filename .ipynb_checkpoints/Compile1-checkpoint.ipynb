{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import regex\n",
    "nlpSpacy = spacy.load('en_core_web_sm')\n",
    "from stanfordcorenlp import StanfordCoreNLP as stnlp\n",
    "nlp = stnlp(r'/home/ayush/stanford-corenlp-full-2018-01-31')\n",
    "from neuralcoref import Coref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################   NER Using SPACY  ###############################\n",
    "def preprocess(sentence):\n",
    "    named_entity = []\n",
    "    \n",
    "    parse_tree = nltk.ne_chunk(nltk.tag.pos_tag(sentence.split()), binary=True)\n",
    "    for tree in parse_tree.subtrees():\n",
    "        if tree.label()=='NE':\n",
    "            entity = \"\"\n",
    "            for t in tree:\n",
    "                entity+=t[0]+\" \"\n",
    "            named_entity.append(entity[:-1])\n",
    "        \n",
    "    return named_entity\n",
    "\n",
    "\n",
    "#######################    All the Proper Nouns   ########################\n",
    "def getAllNNP(sen):\n",
    "    entities = []\n",
    "    for s in sen:\n",
    "        parse = nlp.parse(s)\n",
    "        parse = [str(p).strip() for p in parse.split(\"\\n\")]\n",
    "        for p in parse:\n",
    "            if p.startswith(\"(NP\"):\n",
    "                idxes = [m.start() for m in regex.finditer('\\(NNP ', p)]\n",
    "                if len(idxes)>0:\n",
    "                    prevEnd = -1\n",
    "                    for idx in idxes:\n",
    "                        start = idx+len('\\(NNP ')-1\n",
    "                        end = start+1\n",
    "                        while p[end]!=')':\n",
    "                            end+=1\n",
    "                        if prevEnd+2==idx:\n",
    "                            entities[-1] = entities[-1]+\" \"+p[start:end]\n",
    "                        else:\n",
    "                            entities.append(p[start:end])\n",
    "                        prevEnd = end\n",
    "    \n",
    "    new_entities = []\n",
    "    for ent1 in entities:\n",
    "        count = 0\n",
    "        for ent2 in entities:\n",
    "            if ent1!=ent2:\n",
    "                count+=1 if ent1 in ent2 else 0\n",
    "        if count==0:\n",
    "            new_entities.append(ent1)\n",
    "    return set(new_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########################   Bracket Mapping of a Parse Tree   ################################\n",
    "def getMapping(sentence):\n",
    "    open_bracket = []\n",
    "    mapping = {}\n",
    "    for i,j in enumerate(sentence):\n",
    "        if(j=='('):\n",
    "            open_bracket.append(i)\n",
    "        elif (j==')'):\n",
    "            mapping[open_bracket[-1]] = i\n",
    "            del open_bracket[-1]\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################   SUBJECT ---  VERB --- OBJECT   ###################################\n",
    "def SVO(sentence):\n",
    "    sentence = nlp.parse(sentence)\n",
    "    sentence = str(sentence)\n",
    "    ar = [a.strip() for a in sentence.split('\\n')]\n",
    "    sentence = ''.join(ar)\n",
    "    return getTags(sentence)\n",
    "\n",
    "def getTags(sentence):\n",
    "    mapping = getMapping(sentence)\n",
    "    if '(NP' in sentence:\n",
    "        if '(VP' in sentence:\n",
    "            if sentence.index('(NP')<sentence.index('(VP'):\n",
    "                return getNP(sentence, 0, mapping)\n",
    "            else:\n",
    "                return getVP(sentence, 0, mapping)\n",
    "        else:\n",
    "            return getNP(sentence, 0, mapping)\n",
    "    elif '(VP' in sentence:\n",
    "        return getVP(sentence, 0, mapping)\n",
    "    \n",
    "def getNP(sentence, idxStart, mapping):\n",
    "    res = []\n",
    "    idxNP = sentence.index('(NP')\n",
    "    idxNPend = mapping[idxStart + idxNP] - idxStart\n",
    "    subj = sentence[idxNP:idxNPend]\n",
    "    npAdded = False\n",
    "    \n",
    "    if '(VP' in sentence[idxNP:idxNPend]:\n",
    "        #NP(VP)\n",
    "        remain = sentence[idxNP:idxNPend]\n",
    "        verblist = getVP(remain, idxNP+idxStart, mapping)\n",
    "        if type(verblist) == type('str'):\n",
    "            temp = verblist\n",
    "        else:\n",
    "            temp = verblist[0]\n",
    "        while (type(temp)!=type('str')):\n",
    "            temp = temp[0]\n",
    "        if temp in subj:\n",
    "            idxx = subj.index(temp)\n",
    "            subj = subj[:idxx]\n",
    "        res.append(subj)\n",
    "        res.append(verblist)\n",
    "        npAdded = True    \n",
    "    \n",
    "    remain = sentence[idxNPend+1:]\n",
    "    if '(VP' in remain:\n",
    "        #NP-VP\n",
    "        verblist = getVP(remain, idxNPend+1+idxStart, mapping)\n",
    "        if not npAdded:\n",
    "            res.append(subj)\n",
    "        res.append(verblist)\n",
    "    else:\n",
    "        #NP\n",
    "        if not subj in res:\n",
    "            res.append(subj) \n",
    "    return res\n",
    "        \n",
    "def getVP(sentence, idxStart, mapping):\n",
    "    res = []\n",
    "    idxVP = sentence.index('(VP')\n",
    "    idxVPend = mapping[idxStart + idxVP] - idxStart\n",
    "    verb = sentence[idxVP:idxVPend]\n",
    "    \n",
    "    verbAdded = False\n",
    "    if '(NP' in sentence[idxVP:idxVPend]:\n",
    "        remain = sentence[idxVP:idxVPend]\n",
    "        nplist = getNP(remain, idxVP+idxStart, mapping)\n",
    "        if type(nplist) == type('str'):\n",
    "            temp = nplist\n",
    "        else:\n",
    "            temp = nplist[0]\n",
    "        while (type(temp)!=type('str')):\n",
    "            temp = temp[0]\n",
    "        if temp in verb:\n",
    "            idxx = verb.index(temp)\n",
    "            verb = verb[:idxx]\n",
    "        res.append(verb)\n",
    "        res.append(nplist)\n",
    "        verbAdded = True\n",
    "    \n",
    "    remain = sentence[idxVPend+1:]\n",
    "    if '(NP' in remain:\n",
    "        nplist = getNP(remain, idxVPend+1+idxStart, mapping)\n",
    "        if not verbAdded:\n",
    "            res.append(verb)\n",
    "        res.append(nplist)\n",
    "    else:\n",
    "        if not verb in res:\n",
    "            res.append(verb)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####################################  COREFERENCE RESOLUTION  ##############################\n",
    "def coref(sen):\n",
    "    coref = Coref()\n",
    "    cluster=coref.continuous_coref(utterances=unicode(sen, 'utf-8'))\n",
    "#     print coref.get_resolved_utterances()\n",
    "    return coref.get_resolved_utterances()[0].encode('ascii', 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def kuchbhi(offset,s,mapping):\n",
    "    if(s==''):return ''\n",
    "    if('(' not in s and ')' not in s):\n",
    "        words = s.split(' ')\n",
    "        if(len(words)>1):return words[1]\n",
    "        else: return ''\n",
    "    firstOpenBracket = s.index('(')\n",
    "    correspondingClosingBracket = mapping[firstOpenBracket+offset]-offset\n",
    "    return kuchbhi(offset+firstOpenBracket+1,s[firstOpenBracket+1:correspondingClosingBracket],mapping)+' '+ \\\n",
    "           kuchbhi(correspondingClosingBracket+offset+1,s[correspondingClosingBracket+1:],mapping)\n",
    "    \n",
    "def converter(s):\n",
    "    openCount,closingCount = 0,0\n",
    "    for i in s:\n",
    "        if(i=='('):openCount+=1\n",
    "        elif(i==')'):closingCount+=1\n",
    "    if(openCount>closingCount):\n",
    "        s = s+')'*(openCount-closingCount)\n",
    "    elif(closingCount>openCount):\n",
    "        s = '('*(closingCount-openCount)+s\n",
    "    mapping = getMapping(s)\n",
    "    s = kuchbhi(0,s,mapping)\n",
    "    return ' '.join(s.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####################   GRAPH GENERATION  ########################\n",
    "\n",
    "class EntityNode:\n",
    "    def __init__(self, text):\n",
    "        self.entity = text\n",
    "\n",
    "class NPNode:\n",
    "    \n",
    "    def __init__(self, text, entities):\n",
    "        self.sentence = text\n",
    "        self.before = None\n",
    "        self.entity = None\n",
    "        self.after = None\n",
    "        self.adjacent = []\n",
    "        \n",
    "        haystack = text.split(\".\")[0].split(\" \")\n",
    "        needles = entities\n",
    "        idxes = [(i.entity, haystack.index(i.entity)) for i in needles if i.entity in haystack]\n",
    "        idxes = sorted(idxes, key=lambda x:x[1])\n",
    "        if len(idxes)>0 and len(haystack)>0:\n",
    "            self.before = \" \".join(haystack[:idxes[0][1]])\n",
    "            if len(self.before)==0:\n",
    "                self.before = None\n",
    "            \n",
    "            self.entity = idxes[0][0]\n",
    "            s = \" \".join(haystack[idxes[0][1]+1:])\n",
    "\n",
    "            afterNode = NPNode(s, entities)\n",
    "            if not afterNode.isEmpty():\n",
    "                self.after = afterNode\n",
    "            elif len(s)>0:\n",
    "                self.after = s\n",
    "                \n",
    "    def isEmpty(self):\n",
    "        return self.before==None and self.entity==None and self.after==None\n",
    "    \n",
    "class VPNode:\n",
    "    \n",
    "    def __init__(self, text):\n",
    "        self.verb = text\n",
    "        self.adjacent = []\n",
    "\n",
    "class Graph:\n",
    "        \n",
    "    def __init__(self, entity):\n",
    "        self.NP = []\n",
    "        self.VP = []\n",
    "        self.Edges = []\n",
    "        self.entities = []\n",
    "        for ent in entity:\n",
    "            self.entities.append(EntityNode(ent))\n",
    "    \n",
    "    def addEdgeNPVP(self, npsentence, verb):\n",
    "        npNode = [np for np in self.NP if np.sentence==npsentence]\n",
    "        if npNode is not None and len(npNode)>0:\n",
    "            npNode = npNode[0]\n",
    "        else:\n",
    "            npNode = NPNode(npsentence, self.entities)\n",
    "            self.NP.append(npNode)\n",
    "        \n",
    "#         print verbNode\n",
    "        verbNode = VPNode(verb)\n",
    "        self.VP.append(verbNode)\n",
    "        npNode.adjacent.append(verbNode)\n",
    "        self.Edges.append((npNode, verbNode))\n",
    "        self.Edges.append((verbNode, npNode))\n",
    "#         print verbNode\n",
    "        return verbNode\n",
    "    \n",
    "    def addEdgeVPNP(self, npsentence, verbNode, verb):\n",
    "        npNode = [np for np in self.NP if np.sentence==npsentence]\n",
    "        if npNode is not None and len(npNode)>0:\n",
    "            npNode = npNode[0]\n",
    "        else:\n",
    "            npNode = NPNode(npsentence, self.entities)\n",
    "            self.NP.append(npNode)\n",
    "        \n",
    "        if verbNode is None:\n",
    "            verbNode = VPNode(verb)\n",
    "            self.VP.append(verbNode)\n",
    "        verbNode.adjacent.append(npNode)\n",
    "        self.Edges.append((npNode, verbNode))\n",
    "        self.Edges.append((verbNode, npNode))\n",
    "        \n",
    "    def addEdgeVPVP(self, verb1, verbNode1, verb2):\n",
    "        if verbNode1 is None:\n",
    "            verbNode1 = VPNode(verb1)\n",
    "            self.VP.append(verbNode1)\n",
    "            \n",
    "        verbNode2 = VPNode(verb2)\n",
    "        self.VP.append(verbNode2)\n",
    "        \n",
    "        verbNode1.adjacent.append(verbNode2)\n",
    "        self.Edges.append((verbNode1, verbNode2))\n",
    "        self.Edges.append((verbNode2, verbNode1))\n",
    "        return verbNode2\n",
    "        \n",
    "    \n",
    "    def addEdgeNPNP(self, npsent1, npsent2):\n",
    "        npNode1 = [np for np in self.NP if np.sentence==npsent1]\n",
    "        if npNode1 is not None and len(npNode1)>0:\n",
    "            npNode1 = npNode1[0]\n",
    "        else:\n",
    "            npNode1 = NPNode(npsent1, self.entities)\n",
    "            self.NP.append(npNode1)\n",
    "            \n",
    "        npNode2 = [np for np in self.NP if np.sentence==npsent2]\n",
    "        if npNode2 is not None and len(npNode2)>0:\n",
    "            npNode2 = npNode2[0]\n",
    "        else:\n",
    "            npNode2 = NPNode(npsent2, self.entities)\n",
    "            self.NP.append(npNode2)\n",
    "            \n",
    "        npNode1.adjacent.append(npNode2)\n",
    "        self.Edges.append((npNode1, npNode2))\n",
    "        self.Edges.append((npNode2, npNode1))\n",
    "        \n",
    "    def addNodes(self, relation, vpNode):\n",
    "        if len(relation)<2:\n",
    "            return\n",
    "\n",
    "        for i in range(1, len(relation)):\n",
    "            prev = relation[i-1]\n",
    "            if type(prev)==list:\n",
    "                prev = prev[0]\n",
    "            curr = relation[i]\n",
    "            print prev, \"---->\", curr[0]\n",
    "            if '(NP' in prev:\n",
    "                if '(NP' in curr[0]:\n",
    "                    #NP-NP\n",
    "                    ##INSTEAD OF prev and curr[0] send converter(prev) and converter(curr[0])\n",
    "                    self.addEdgeNPNP(converter(prev), converter(curr[0]))\n",
    "                else:\n",
    "                    #NP-VP\n",
    "                    vpNode = self.addEdgeNPVP(converter(prev), converter(curr[0]))\n",
    "#                     print vpNode.verb\n",
    "            else:\n",
    "                if '(NP' in curr[0]:\n",
    "                    #VP-NP\n",
    "                    self.addEdgeVPNP(converter(curr[0]), vpNode, converter(prev))\n",
    "                else:\n",
    "                    #VP-VP\n",
    "                    vpNode = self.addEdgeVPVP(converter(prev), vpNode, converter(curr[0]))\n",
    "\n",
    "            self.addNodes(curr, vpNode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getGraph(sen):\n",
    "    \n",
    "    replace = [\"\\'s\", \"\\'\", \"\\\"\"]\n",
    "    sen = regex.sub(r' \\(.*\\)', '', sen)\n",
    "    sen = regex.sub(r'\\(.*\\)', '', sen)\n",
    "\n",
    "    for r in replace:\n",
    "        sen = sen.replace(r, \"\")\n",
    "    sen = coref(sen)\n",
    "    \n",
    "    sen = sen.split('.')\n",
    "    del sen[-1]\n",
    "    \n",
    "    ner = list(getAllNNP(sen))\n",
    "    for i in range(len(sen)):\n",
    "        for n in ner:\n",
    "            sen[i] = sen[i].replace(n.lower(), n)\n",
    "            sen[i] = sen[i].strip()\n",
    "    print sen\n",
    "\n",
    "\n",
    "    for i in range(len(sen)):\n",
    "        sen[i] = regex.sub(r' \\(.*\\)', '', sen[i])\n",
    "        sen[i] = regex.sub(r'\\(.*\\)', '', sen[i])\n",
    "        for r in replace:\n",
    "            sen[i] = sen[i].replace(r, \"\")\n",
    "        sen[i] = \" \".join(sen[i].split())\n",
    "    entities = []\n",
    "    for s in sen:\n",
    "        entities.extend(preprocess(s))\n",
    "    entities = set(entities)\n",
    "   \n",
    "    svo = []\n",
    "    for s in sen:\n",
    "        svo.append(SVO(s))\n",
    "        \n",
    "        \n",
    "    graph = Graph(ner)\n",
    "    for rel in svo:\n",
    "        graph.addNodes(rel, None)\n",
    "        \n",
    "    return graph, ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings from /usr/local/lib/python2.7/dist-packages/neuralcoref/weights/static_word\n",
      "Loading embeddings from /usr/local/lib/python2.7/dist-packages/neuralcoref/weights/tuned_word\n",
      "['Dashrath was the king of Ayodhya', 'Dashrath had three wives namely Kausalya, Kaikeyi and Sumitra', 'Bharat and Shatrugan were the sons of Kaikeyi', 'Ram was the son of Kausalya', 'Laxman was the son of Sumitra', 'Ram was married to Sita', 'Sita was the daughter of Janak', 'Ravan was the ruler of Lanka', 'Ravan kidnapped Sita', 'Ram killed Ravan']\n",
      "(NP (NNP Dashrath) ----> (VP (VBD was)\n",
      "(VP (VBD was) ----> (NP(NP (DT the) (NN king))(PP (IN of)(NP (NNP Ayodhya)))\n",
      "(NP (NNP Dashrath) ----> (VP (VBD had)\n",
      "(VP (VBD had) ----> (NP(NP (CD three) (NNS wives))(ADVP (RB namely))(SBAR(S(NP (NNP Kausalya) (, ,) (NNP Kaikeyi)(CC and)(NNP Sumitra))))\n",
      "(NP (NNP Bharat)(CC and)(NNP Shatrugan) ----> (VP (VBD were)\n",
      "(VP (VBD were) ----> (NP(NP (DT the) (NNS sons))(PP (IN of)(NP (NNP Kaikeyi)))\n",
      "(NP (NNP Ram) ----> (VP (VBD was)\n",
      "(VP (VBD was) ----> (NP(NP (DT the) (NN son))(PP (IN of)(NP (NNP Kausalya)))\n",
      "(NP (NNP Laxman) ----> (VP (VBD was)\n",
      "(VP (VBD was) ----> (NP(NP (DT the) (NN son))(PP (IN of)(NP (NNP Sumitra)))\n",
      "(NP (NNP Ram) ----> (VP (VBD was)(VP (VBN married)(PP (TO to)\n",
      "(VP (VBD was)(VP (VBN married)(PP (TO to) ----> (NP (NNP Sita)\n",
      "(NP (NNP Sita) ----> (VP (VBD was)\n",
      "(VP (VBD was) ----> (NP(NP (DT the) (NN daughter))(PP (IN of)(NP (NNP Janak)))\n",
      "(NP (NNP Ravan) ----> (VP (VBD was)\n",
      "(VP (VBD was) ----> (NP(NP (DT the) (NN ruler))(PP (IN of)(NP (NNP Lanka)))\n",
      "(NP (NNP Ravan) ----> (VP (VBD kidnapped)\n",
      "(VP (VBD kidnapped) ----> (NP (NNP Sita)\n",
      "(NP (NNP Ram) ----> (VP (VBD killed)\n",
      "(VP (VBD killed) ----> (NP (NNP Ravan)\n"
     ]
    }
   ],
   "source": [
    "sen = \"Dashrath was the king of Ayodhya. He had three wives namely Kausalya, Kaikeyi and Sumitra. Bharat and Shatrugan were the sons of Kaikeyi. Ram was the son of Kausalya. Laxman was the son of Sumitra. Ram was married to Sita. She was the daughter of Janak. Ravan was the ruler of Lanka. He kidnapped Sita. Ram killed him.\"\n",
    "graph, ner = getGraph(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NP Sentence :  Dashrath\n",
      "NP adjacent :  was ,  had , \n",
      "-------------------------------------------------------------------------\n",
      "NP Sentence :  the king of Ayodhya\n",
      "NP adjacent : \n",
      "-------------------------------------------------------------------------\n",
      "NP Sentence :  three wives namely Kausalya , Kaikeyi and Sumitra\n",
      "NP adjacent : \n",
      "-------------------------------------------------------------------------\n",
      "NP Sentence :  Bharat and Shatrugan\n",
      "NP adjacent :  were , \n",
      "-------------------------------------------------------------------------\n",
      "NP Sentence :  the sons of Kaikeyi\n",
      "NP adjacent : \n",
      "-------------------------------------------------------------------------\n",
      "NP Sentence :  Ram\n",
      "NP adjacent :  was ,  was married to ,  killed , \n",
      "-------------------------------------------------------------------------\n",
      "NP Sentence :  the son of Kausalya\n",
      "NP adjacent : \n",
      "-------------------------------------------------------------------------\n",
      "NP Sentence :  Laxman\n",
      "NP adjacent :  was , \n",
      "-------------------------------------------------------------------------\n",
      "NP Sentence :  the son of Sumitra\n",
      "NP adjacent : \n",
      "-------------------------------------------------------------------------\n",
      "NP Sentence :  Sita\n",
      "NP adjacent :  was , \n",
      "-------------------------------------------------------------------------\n",
      "NP Sentence :  the daughter of Janak\n",
      "NP adjacent : \n",
      "-------------------------------------------------------------------------\n",
      "NP Sentence :  Ravan\n",
      "NP adjacent :  was ,  kidnapped , \n",
      "-------------------------------------------------------------------------\n",
      "NP Sentence :  the ruler of Lanka\n",
      "NP adjacent : \n",
      "-------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for np in graph.NP:\n",
    "    print \"NP Sentence : \" , np.sentence\n",
    "    \n",
    "    print \"NP adjacent : \", \n",
    "    for vp in np.adjacent:\n",
    "        try:\n",
    "            print vp.verb , \", \" ,\n",
    "        except:\n",
    "            print vp.sentence, \", \" ,\n",
    "    print\n",
    "    print \"-------------------------------------------------------------------------\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VP Sentence :  was\n",
      "VP Adjacent :  the king of Ayodhya\n",
      "----------------------------------------------------------------------------------------------------\n",
      "VP Sentence :  had\n",
      "VP Adjacent :  three wives namely Kausalya , Kaikeyi and Sumitra\n",
      "----------------------------------------------------------------------------------------------------\n",
      "VP Sentence :  were\n",
      "VP Adjacent :  the sons of Kaikeyi\n",
      "----------------------------------------------------------------------------------------------------\n",
      "VP Sentence :  was\n",
      "VP Adjacent :  the son of Kausalya\n",
      "----------------------------------------------------------------------------------------------------\n",
      "VP Sentence :  was\n",
      "VP Adjacent :  the son of Sumitra\n",
      "----------------------------------------------------------------------------------------------------\n",
      "VP Sentence :  was married to\n",
      "VP Adjacent :  Sita\n",
      "----------------------------------------------------------------------------------------------------\n",
      "VP Sentence :  was\n",
      "VP Adjacent :  the daughter of Janak\n",
      "----------------------------------------------------------------------------------------------------\n",
      "VP Sentence :  was\n",
      "VP Adjacent :  the ruler of Lanka\n",
      "----------------------------------------------------------------------------------------------------\n",
      "VP Sentence :  kidnapped\n",
      "VP Adjacent :  Sita\n",
      "----------------------------------------------------------------------------------------------------\n",
      "VP Sentence :  killed\n",
      "VP Adjacent :  Ravan\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for vp in graph.VP:\n",
    "    print \"VP Sentence : \", vp.verb\n",
    "    print \"VP Adjacent : \", \n",
    "    for np in vp.adjacent:\n",
    "        try:\n",
    "            print np.sentence, \n",
    "        except:\n",
    "            print np.verb, \n",
    "    print \n",
    "    print \"-\"*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ROOT\n",
      "  (S\n",
      "    (NP (NNP Ravan))\n",
      "    (VP (VBD was)\n",
      "      (VP (VBN killed)\n",
      "        (PP (IN by)\n",
      "          (NP (WP whom)))))\n",
      "    (. .)))\n",
      "Loading embeddings from /usr/local/lib/python2.7/dist-packages/neuralcoref/weights/static_word\n",
      "Loading embeddings from /usr/local/lib/python2.7/dist-packages/neuralcoref/weights/tuned_word\n",
      "['Ravan was killed by whom']\n",
      "(NP (NNP Ravan) ----> (VP (VBD was)(VP (VBN killed)(PP (IN by)\n",
      "(VP (VBD was)(VP (VBN killed)(PP (IN by) ----> (NP (WP whom)\n"
     ]
    }
   ],
   "source": [
    "ques = \"Ravan was killed by whom.\"\n",
    "print nlp.parse(ques)\n",
    "graphQues = getGraph(ques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(NP (NNP Ravan)', ['(VP (VBD was)(VP (VBN killed)(PP (IN by)', ['(NP (WP whom)']]]\n"
     ]
    }
   ],
   "source": [
    "print SVO(ques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashrath\n",
      "the king of Ayodhya\n",
      "three wives namely Kausalya , Kaikeyi and Sumitra\n",
      "Bharat and Shatrugan\n",
      "the sons of Kaikeyi\n",
      "Ram\n",
      "the son of Kausalya\n",
      "Laxman\n",
      "the son of Sumitra\n",
      "Sita\n",
      "the daughter of Janak\n",
      "Ravan\n",
      "the ruler of Lanka\n"
     ]
    }
   ],
   "source": [
    "ent = 'Ram'\n",
    "for np in graph.NP:\n",
    "    print np.sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "entToNode = {}\n",
    "for np in graph.NP:\n",
    "    if np.entity is not None:\n",
    "        entToNode[np.entity] = np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def stopWordRemoval(ques):\n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "    whWords = ['Who', 'Whom', 'What', 'Where', 'Which', 'How', 'Why', 'When']\n",
    "    for wh in whWords:\n",
    "        try:\n",
    "            idx = stop_words.index(wh)\n",
    "            del stop_words[idx]\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            idx = stop_words.index(wh.lower())\n",
    "            del stop_words[idx]\n",
    "        except:\n",
    "            pass\n",
    "    print ques\n",
    "    return [q for q in ques.split() if q not in stop_words]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ROOT\n",
      "  (SBARQ\n",
      "    (WHNP (WP Who))\n",
      "    (SQ (VBD was)\n",
      "      (NP\n",
      "        (NP (DT the) (NN king))\n",
      "        (PP (IN of)\n",
      "          (NP (NNP Ayodhya)))))\n",
      "    (. .)))\n"
     ]
    }
   ],
   "source": [
    "ques = 'Who was the king of Ayodhya.'\n",
    "print nlp.parse(ques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def findAns(ques):\n",
    "    graphQues, nerQues = getGraph(ques)\n",
    "    quesEntity = nerQues[0]\n",
    "    entNode = entToNode[quesEntity]\n",
    "    ques = stopWordRemoval(ques[:-1])\n",
    "    origQues = ' '.join(ques)\n",
    "    sentence = nlp.parse(origQues)\n",
    "    sentence = str(sentence)\n",
    "    ar = [a.strip() for a in sentence.split('\\n')]\n",
    "    sentence = ''.join(ar)\n",
    "    wpIdx = sentence.find('(WP')\n",
    "    mapping = getMapping(sentence)\n",
    "    wp = converter(sentence[wpIdx:mapping[wpIdx]])\n",
    "    j = ques.index(wp)\n",
    "    entityNode = entToNode[quesEntity]\n",
    "    \n",
    "    \n",
    "    i = ques.index(quesEntity)\n",
    "    deli = 1\n",
    "    if(j<i):deli = -1\n",
    "    while (i-j)!=1 and (i-j)!=-1: \n",
    "        print entityNode.sentence\n",
    "        input()\n",
    "        if(ques[i+deli] in entityNode.sentence):\n",
    "            i+=deli\n",
    "        else:\n",
    "            for edge in graph.Edges:\n",
    "                print entityNode.sentence\n",
    "                if entityNode==edge[1]:\n",
    "                    node = edge[0]\n",
    "                    if(node in graph.NP):\n",
    "                        if(ques[i+deli] in node.sentence):\n",
    "                            i+=deli\n",
    "                            entityNode = node\n",
    "                            break\n",
    "                    else:\n",
    "                        if(ques[i+deli] in node.verb):\n",
    "                            i+=deli\n",
    "                            entityNode = node\n",
    "                            break\n",
    "    if(entityNode in graph.NP):\n",
    "        for edge in graph.Edges:\n",
    "            if(entityNode==edge[1] and (edge[0] not in graph.NP)):\n",
    "                entityNode = edge[0]\n",
    "                break\n",
    "    \n",
    "    for edge in graph.Edges:\n",
    "        if(entityNode==edge[1] and (edge[0] in graph.NP)):\n",
    "            entityNode = edge[0]\n",
    "            break\n",
    "    return entityNode.entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings from /usr/local/lib/python2.7/dist-packages/neuralcoref/weights/static_word\n",
      "Loading embeddings from /usr/local/lib/python2.7/dist-packages/neuralcoref/weights/tuned_word\n",
      "['Who was the king of Ayodhya']\n",
      "Who was the king of Ayodhya\n",
      "the king of Ayodhya\n",
      "1\n",
      "Dashrath\n"
     ]
    }
   ],
   "source": [
    "print findAns(ques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
